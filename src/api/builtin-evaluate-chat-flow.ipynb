{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-ai-evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from contoso_chat.chat_request import get_response\n",
    "# Import the Relevanace and Groundedness evaluators\n",
    "from azure.ai.evaluation import RelevanceEvaluator, GroundednessEvaluator, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    data_path = \"./evaluators/data.jsonl\"\n",
    "\n",
    "    df = pd.read_json(data_path, lines=True)\n",
    "    df.head()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_response_data(df):\n",
    "    results = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        customerId = row['customerId']\n",
    "        question = row['question']\n",
    "        \n",
    "        # Run contoso-chat/chat_request flow to get response\n",
    "        response = get_response(customerId=customerId, question=question, chat_history=[])\n",
    "        print(response)\n",
    "        \n",
    "        # Add results to list\n",
    "        result = {\n",
    "            'question': question,\n",
    "            'context': response[\"context\"],\n",
    "            'answer': response[\"answer\"]\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "    # Save results to a JSONL file\n",
    "    with open('result.jsonl', 'w') as file:\n",
    "        for result in results:\n",
    "            file.write(json.dumps(result) + '\\n')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This needs to be re-written so that you don't need to hardcode these values.\n",
    "# For testing purposes, I had mine hardcoded.\n",
    "model_config = {\n",
    "    \"azure_endpoint\": \"<your azure endoint>\",\n",
    "    \"api_key\": \"<your deployment API key>\",\n",
    "    \"azure_deployment\": \"gpt-4-evals\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialze Relevance and Groundedness Evaluators\n",
    "relevance_eval = RelevanceEvaluator(model_config)\n",
    "groundedness_eval = GroundednessEvaluator(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the data to be evaluated\n",
    "data_path = \"result.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evaluation():\n",
    "    evaluate(\n",
    "    data=data_path, # provide your data here\n",
    "    evaluators={\n",
    "        \"relevance\": relevance_eval,\n",
    "        \"groundedness\": groundedness_eval,\n",
    "    },\n",
    "    # column mapping\n",
    "    evaluator_config={\n",
    "        # A config is needed for groundedness, which requires response and context\n",
    "        # Only the values data or target are allowed below\n",
    "        \"groundedness\": {\n",
    "            \"response\": \"${data.answer}\",\n",
    "            \"context\": \"${data.context}\"\n",
    "        },\n",
    "        # A config is needed for relevance, which requires response, context, and query\n",
    "        # Only the values data or target are allowed below\n",
    "        \"relevance\": {\n",
    "            \"response\": \"${data.answer}\",\n",
    "            \"context\": \"${data.context}\",\n",
    "            \"query\": \"${data.question}\"\n",
    "        }\n",
    "    },\n",
    "    # Optionally provide an output path to dump a json of metric summary, row level data and metric and studio URL\n",
    "    # I've only got this to output the results to a file once, otherwise I view the results in Trace view\n",
    "    # A link to Trace View will appear after you run the next cell\n",
    "    # For an example of an outfit for the RevelanceEvalutor, view the file builtin-evals-results-sample.json\n",
    "    output_path=\"./myevalresults.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you run and get a Key error, close the Codespace and restart. Then come back and run the evaluation again.\n",
    "# The run takes around 4 minutes or so with two evaluators, possibly longer depending on retries for quota limits.\n",
    "# If a markdown version of the results is preferred, then the logic will need to be added to do so.\n",
    "# For now, I only get the results for the metrics just to validate that this works.\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "   test_data_df = load_data()\n",
    "   response_results = create_response_data(test_data_df)\n",
    "   result_evaluated = get_evaluation()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
